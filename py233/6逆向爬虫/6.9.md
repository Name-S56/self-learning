 重新定义 scrapy原来对于start_urls的处理**

  **重写start_requests()即可**

![image-20240305162422415](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240305162422415.png)

![image-20240305162613273](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240305162613273.png)

```
def start_requests(self):
        lst = cookie_str.split(";")
         dic = {}
         for it in lst:
             k,v = it.split("=")
             dic[k.strip()] = v.strip()

		yield scrapy.Request(
             url= self.start_urls[0],
             cookies=dic)
```

上半部**字符串切割**



scrapy crawl login 运行

上面方法不好---下面走登陆流程

![image-20240305163949588](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240305163949588.png)

# 中间件

scrapy startproject mid

scrapy genspider baidu baidu.com

setting.py内

**ROBOTSTXT_OBEY** **=** **False**

**LOG_LEVEL** **=** **"WARNING"**

scrapy crawl baidu

middlewares.py 中间件

![image-20240305165513404](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240305165513404.png)

先看下载器中间件

 

```
# Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.
```

重点在 process_request 

在引擎将请求的信息交给下载器之前，自动的调用该方法

process_response..

process_exception 异常 (看名就知道了...)

spider_open 爬虫开始 

**setting.py**  DOWNLOADER_MIDDLEWARES

![运行顺序](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240305170646902.png)

**UA、代理**处理---process_request

process_request 返回值有规定

> 1. 如果返回的    None，不做拦截,继续向后面的中间件执行.(多个中间件,权重大越往后)
> 2. 如果返回的是Request．后续的中间件将不再执行.将请求重新交给引擎．引擎重新扔给调度器
> 3. 如果返回的是Response，后续的中间件将不再执行.将响应信息交给引擎，引擎将响应丢给spider.进行数据处理![image-20240305171449653](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240305171449653.png)

一个请求return 	；yield一群

弄2个中间件？？？e.g.权重544 545

## UA随机

















