# 爬虫工程化

open source & 框架extracting data from websites
fast simple extensible

![image-20240201185936857](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240201185936857.png)

![image-20240201190010304](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240201190010304.png)

1. 爬虫中起始的url构造成request对象,并传递给调度器.
2. 引擎从调度器中获取到request对象.然后交给下载器
3. 由下载器来获取到页面源代码,并封装成response对象.并回馈给引擎
4. 引擎将获取到的response对象传递给spider ,由 spider对数据进行解析(parse).并回馈给引擎
5. 引擎将数据传递给pipeline进行数据持久化保存或进一步的数据处理.
6. 在此期间如果spider中提取到的并不是数据.而是子页面url.可以进一步提交给调度器,进而重复步骤2的过程

外面game 整个项目   内game文件夹 --根目录

cfg设置   部署用

 **spiders** 

items.py  封装大数据
middlewares.py   pass中间件
**pipeline** 存储
settings 配置

`scrapy genspider xiao 4399.com`  生成一个xiao.py

![image-20240201202236755](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240201202236755.png)

运行scrapy`scrapy crawl xiao`   出现日志

可在settings.py中更改  -- LOG_LEVEL ="WARNING"
**日志级别  DEBUG , INFO , WARNING , ERROR , CRITICAL**

```python
def parse(self, response):  #该方法默认处理解析
        print(response) #响应对象
```


- 之前![58bea1ad5a411b22c29b31261a546db](C:\Users\a2950\AppData\Local\Temp\WeChat Files\58bea1ad5a411b22c29b31261a546db.png)
- 之后![image-20240201203022826](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240201203022826.png)

`response.xpath("//ul[@class='n-game cf']/li/a/b/text()") #game name`

![image-20240201204324575](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240201204324575.png)

`txt = response.xpath("//ul[@class='n-game cf']/li/a/b/text()").extract()`

![image-20240201205359535](C:\Users\a2950\AppData\Roaming\Typora\typora-user-images\image-20240201205359535.png)

`xpath("./a/b/text()").extract_first() `--None

parse  overwrrides重写

末尾pipeline  转到pipeline文件

**settings文件:**
ITEM_PIPELINES = {
    #key 就是管道的路径
    #value  优先级
    "game.pipelines.GamePipeline": 300,
}





# 模板套路

1. 创建项目

   scrapy startproject 项目名

2. 进入项目

   cd 项目名

3. 创建爬虫

   scrapy genspidr 名字 域名

4. 可能需要修改start_urls,

5. 数据解析    用parse(response)

   ​	def	parse (self, response):
            response.text
            response.xpath()
            response.css()

            解析数据  xpath()默认返回Selector
            extract() 返回列表
            extract_first()返回一个数据

            yield    -->pipeline
6. pipeline 数据存储
    class 类名():
        def process_item(self, item ,spider):
        item:数据
        spider:爬虫
        return item         #必须return,否则下一个管道收不到
7. 设置settings.py pipeline生效
        ITEM_PIPELINES = {
        #key 就是管道的路径
        #value  优先级
        "game.pipelines.GamePipeline": 300,
        }
8. 运行爬虫
    scrapy crawl 爬虫名
            